{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Evaluation Metrics ‚Äî Top-k, Perplexity, BLEU, ROUGE"
      ],
      "metadata": {
        "id": "QqKQqTbRvhCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Top-k Accuracy**\n",
        "**What:** Measures how often the correct class is among the model‚Äôs top-k predictions.  \n",
        "**Formula:**  \n",
        "$$\n",
        "\\text{Top-k Accuracy} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{1}(y_i \\in \\text{Top-k}(\\hat{y}_i))\n",
        "$$  \n",
        "**Where used:**  \n",
        "- Image classification, recommendation systems, retrieval tasks.  \n",
        "- Top-1 = normal accuracy; higher k captures model uncertainty.  \n",
        "\n",
        "**Limitations:**  \n",
        "- Ignores ranking within top-k (e.g., being 2nd or 5th counts the same).  \n",
        "- Not suitable for regression or sequence generation."
      ],
      "metadata": {
        "id": "KqL7-iEZvnAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lch0ebxmvYCZ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from math import exp, log\n",
        "from typing import List, Iterable, Tuple\n",
        "\n",
        "\n",
        "def top_k_accuracy(y_true: List[int], y_pred_scores: List[Iterable[float]], k: int = 5) -> float:\n",
        "    \"\"\"\n",
        "    Top-k accuracy: fraction of examples where the true class index is among the top-k predicted scores.\n",
        "    - y_true: list of true class indices (len = N)\n",
        "    - y_pred_scores: list of score vectors (logits or probs) per example (shape N x C)\n",
        "    - k: top-k threshold\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "\n",
        "    for t, scores in zip(y_true, y_pred_scores):\n",
        "        topk = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "        correct += int(t in topk)\n",
        "\n",
        "    return correct / len(y_true) if y_true else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Perplexity**\n",
        "**What:** Measures how ‚Äúsurprised‚Äù a language model is by the data. Lower = better.  \n",
        "**Formula:**  \n",
        "$$\n",
        "\\text{PPL} = e^{-\\frac{1}{T} \\sum_{t=1}^{T} \\log p(w_t | w_{<t})}\n",
        "$$  \n",
        "**Where used:**  \n",
        "- Evaluating language models (GPT, RNN, etc.) on next-token prediction tasks.  \n",
        "\n",
        "**Limitations:**  \n",
        "- Only works when you can compute exact probabilities.  \n",
        "- Doesn‚Äôt always correlate with human text quality.  \n",
        "- Sensitive to tokenization."
      ],
      "metadata": {
        "id": "uNo33EHTvsQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_from_nll(nll_tokens: List[float]) -> float:\n",
        "    \"\"\"\n",
        "    Perplexity from per-token negative log-likelihoods (natural log).\n",
        "    PPL = exp( mean(NLL) )\n",
        "    - nll_tokens: list of -log p(w_t | context) for each token (base e)\n",
        "    \"\"\"\n",
        "    if not nll_tokens:\n",
        "        return float('nan')\n",
        "    return exp(sum(nll_tokens) / len(nll_tokens))\n",
        "\n",
        "\n",
        "def perplexity_from_logprobs(logprobs: List[float]) -> float:\n",
        "    \"\"\"\n",
        "    Perplexity when given per-token log-probabilities (natural log).\n",
        "    - logprobs: list of log p(w_t | context)\n",
        "    \"\"\"\n",
        "    if not logprobs:\n",
        "        return float('nan')\n",
        "    mean_nll = -sum(logprobs) / len(logprobs)\n",
        "    return exp(mean_nll)"
      ],
      "metadata": {
        "id": "4BspBadJv5jB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [2, 0]\n",
        "y_pred = [[0.1, 0.2, 0.7], [0.6, 0.3, 0.1]]\n",
        "print(\"Top-1 acc:\", top_k_accuracy(y_true, y_pred, k=1))  # 1.0\n",
        "print(\"Top-2 acc:\", top_k_accuracy(y_true, y_pred, k=2))  # 1.0\n",
        "print(\"PPL:\", perplexity_from_logprobs([-0.2, -0.1, -0.3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrS5h699vuoZ",
        "outputId": "ce0bbfff-72ee-4f66-b4c2-d32f477d88c2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 acc: 1.0\n",
            "Top-2 acc: 1.0\n",
            "PPL: 1.2214027581601699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3. BLEU (Bilingual Evaluation Understudy)**\n",
        "**What:** Precision-based metric comparing n-gram overlap between candidate and reference text.  \n",
        "**Formula:**  \n",
        "$$\n",
        "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
        "$$  \n",
        "where  \n",
        "$$\n",
        "p_n = \\frac{\\text{\\# matched n-grams}}{\\text{\\# candidate n-grams}}, \\quad\n",
        "\\text{BP} =\n",
        "\\begin{cases}\n",
        "1, & c > r \\\\\n",
        "e^{1 - \\frac{r}{c}}, & c \\le r\n",
        "\\end{cases}\n",
        "$$  \n",
        "**Where used:**  \n",
        "- Machine translation, summarization, text generation.\n",
        "\n",
        "**Limitations:**  \n",
        "- Ignores recall (only precision).  \n",
        "- Penalizes valid paraphrases (exact n-gram match only).  \n",
        "- Poor at sentence-level evaluation; better averaged over corpus."
      ],
      "metadata": {
        "id": "mS_kcCGNwDbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# BLEU (sentence-level, single candidate, multi-reference)\n",
        "# -----------------------------\n",
        "\n",
        "def _ngrams(tokens: List[str], n: int) -> Counter:\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)) if n > 0 else Counter()\n",
        "\n",
        "def _brevity_penalty(c_len: int, r_len: int) -> float:\n",
        "    if c_len == 0:\n",
        "        return 0.0\n",
        "    return 1.0 if c_len > r_len else exp(1 - r_len / c_len)\n",
        "\n",
        "def bleu(candidate: List[str], references: List[List[str]], max_n: int = 4, smooth: bool = True) -> float:\n",
        "    \"\"\"\n",
        "    BLEU (Papineni et al.): geometric mean of clipped n-gram precisions * brevity penalty.\n",
        "    - candidate: tokenized hypothesis\n",
        "    - references: list of tokenized references\n",
        "    - max_n: up to n-gram order (default 4)\n",
        "    - smooth: add-1 smoothing for zero counts (Chen & Cherry style)\n",
        "    \"\"\"\n",
        "    if not candidate:\n",
        "        return 0.0\n",
        "\n",
        "    # choose reference length closest to candidate (ties -> shortest)\n",
        "    c_len = len(candidate)\n",
        "    r_lens = [len(r) for r in references]\n",
        "    r_len = min(r_lens, key=lambda rl: (abs(rl - c_len), rl))\n",
        "\n",
        "    precisions = []\n",
        "    for n in range(1, max_n + 1):\n",
        "        c_grams = _ngrams(candidate, n)\n",
        "        max_ref = Counter()\n",
        "        for r in references:\n",
        "            max_ref |= _ngrams(r, n)  # elementwise max for clipped counts\n",
        "        overlap = sum((c_grams & max_ref).values())\n",
        "        total = max(1, sum(c_grams.values()))\n",
        "        if overlap == 0 and smooth:\n",
        "            # add-1 smoothing\n",
        "            p_n = (overlap + 1) / (total + 1)\n",
        "        else:\n",
        "            p_n = overlap / total\n",
        "        precisions.append(p_n)\n",
        "\n",
        "    # geometric mean in log space\n",
        "    # If any precision is zero and no smoothing, BLEU becomes 0 (handled naturally).\n",
        "    log_prec = sum((0 if p == 0 else log(p)) for p in precisions) / max_n\n",
        "    bp = _brevity_penalty(c_len, r_len)\n",
        "    return bp * exp(log_prec)"
      ],
      "metadata": {
        "id": "fMc-_QeIvqEV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU\n",
        "cand = \"the cat is on the mat\".split()\n",
        "refs = [\"there is a cat on the mat\".split(), \"a cat is on the mat\".split()]\n",
        "print(\"BLEU:\", bleu(cand, refs, max_n=4, smooth=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Petxy8wGtV",
        "outputId": "c235cadf-8a11-4a99-a67e-4ce410475b98"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 0.7598356856515925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
        "**What:** Recall-based metric measuring overlap between candidate and reference summaries.  \n",
        "- **ROUGE-N:** n-gram overlap (recall, precision, F1).  \n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{overlap}}{\\text{ref total}}, \\quad\n",
        "  \\text{Precision} = \\frac{\\text{overlap}}{\\text{cand total}}, \\quad\n",
        "  F1 = \\frac{2PR}{P+R}\n",
        "  $$\n",
        "- **ROUGE-L:** Based on Longest Common Subsequence (LCS).  \n",
        "\n",
        "**Where used:**  \n",
        "- Summarization, text generation, dialogue response quality.  \n",
        "\n",
        "**Limitations:**  \n",
        "- Overly lexical (word-form sensitive).  \n",
        "- Doesn‚Äôt capture meaning or fluency.  \n",
        "- High recall may come from redundant text."
      ],
      "metadata": {
        "id": "PJpDYzidwLRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# ROUGE-1 / ROUGE-2 (F1, Precision, Recall) and ROUGE-L (F1)\n",
        "# -----------------------------\n",
        "def _overlap_counts(cand: List[str], ref: List[str], n: int) -> Tuple[int, int, int]:\n",
        "    \"\"\"Return (overlap, cand_total, ref_total) for ROUGE-n using multiset overlap.\"\"\"\n",
        "    c_grams = _ngrams(cand, n)\n",
        "    r_grams = _ngrams(ref, n)\n",
        "    overlap = sum((c_grams & r_grams).values())\n",
        "    return overlap, sum(c_grams.values()), sum(r_grams.values())\n",
        "\n",
        "def _prf1(overlap: int, cand_total: int, ref_total: int) -> Tuple[float, float, float]:\n",
        "    p = overlap / cand_total if cand_total else 0.0\n",
        "    r = overlap / ref_total if ref_total else 0.0\n",
        "    f1 = 2*p*r / (p + r) if (p + r) > 0 else 0.0\n",
        "    return p, r, f1\n",
        "\n",
        "def rouge_n(candidate: List[str], reference: List[str], n: int = 1) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    ROUGE-n (n=1 or 2 typical): multiset n-gram overlap.\n",
        "    Returns (precision, recall, F1).\n",
        "    \"\"\"\n",
        "    overlap, c_tot, r_tot = _overlap_counts(candidate, reference, n)\n",
        "    return _prf1(overlap, c_tot, r_tot)\n",
        "\n",
        "def _lcs_len(a: List[str], b: List[str]) -> int:\n",
        "    # O(|a|*|b|) ie O(n2) but won't be issue for this scale\n",
        "    m, n = len(a), len(b)\n",
        "    dp = [0]*(n+1)\n",
        "    for i in range(1, m+1):\n",
        "        prev = 0\n",
        "        for j in range(1, n+1):\n",
        "            temp = dp[j]\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j-1])\n",
        "            prev = temp\n",
        "    return dp[-1]\n",
        "\n",
        "def rouge_l(candidate: List[str], reference: List[str]) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    ROUGE-L: based on Longest Common Subsequence.\n",
        "    Returns (precision, recall, F1) where overlap = LCS length.\n",
        "    \"\"\"\n",
        "    lcs = _lcs_len(candidate, reference)\n",
        "    p = lcs / len(candidate) if candidate else 0.0\n",
        "    r = lcs / len(reference) if reference else 0.0\n",
        "    f1 = 2*p*r / (p + r) if (p + r) > 0 else 0.0\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "6Iar-hMIwIqf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE\n",
        "ref = \"a cat sat on the mat\".split()\n",
        "print(\"ROUGE-1 (P,R,F1):\", rouge_n(cand, ref, n=1))\n",
        "print(\"ROUGE-2 (P,R,F1):\", rouge_n(cand, ref, n=2))\n",
        "print(\"ROUGE-L (P,R,F1):\", rouge_l(cand, ref))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y777yeDvwPIK",
        "outputId": "c245c9da-c13b-473c-f444-04c3698881d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 (P,R,F1): (0.6666666666666666, 0.6666666666666666, 0.6666666666666666)\n",
            "ROUGE-2 (P,R,F1): (0.4, 0.4, 0.4000000000000001)\n",
            "ROUGE-L (P,R,F1): (0.6666666666666666, 0.6666666666666666, 0.6666666666666666)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Quick Mental Model\n",
        "| Metric | Domain | Focus | High Score Means | Weakness |\n",
        "|:--|:--|:--|:--|:--|\n",
        "| **Top-k Accuracy** | Classification | Rank of true label | Model often includes truth | Ignores rank order |\n",
        "| **Perplexity** | Language modeling | Predictive confidence | Model predicts next word well | Not aligned with human quality |\n",
        "| **BLEU** | Translation | n-gram precision | Candidate close to reference | Misses paraphrases |\n",
        "| **ROUGE** | Summarization | n-gram recall / LCS | Captures key content | Surface-form only |"
      ],
      "metadata": {
        "id": "5JtMtkJ_wTaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cross-Validation (K-Fold) with Dummy Dataset\n",
        "\n",
        "In this notebook, we explore **K-Fold Cross-Validation**, a robust technique for evaluating model performance by repeatedly training and testing on different partitions of the dataset. Instead of relying on a single train/test split, K-Fold validation divides the data into *K equal folds* and iteratively uses each fold as the test set while the remaining folds serve as the training set.\n",
        "\n",
        "**Why it matters:**\n",
        "- Reduces variance in evaluation results.\n",
        "- Utilizes all data for both training and validation.\n",
        "- Provides a better estimate of generalization performance.\n",
        "\n",
        "We'll:\n",
        "1. Generate a dummy dataset using `make_classification`.\n",
        "2. Implement K-Fold cross-validation using `sklearn.model_selection.KFold`.\n",
        "3. Train a simple model (e.g., Logistic Regression).\n",
        "4. Compute and interpret average accuracy across folds.\n"
      ],
      "metadata": {
        "id": "XcnpPqlpwXKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross Validation with Dummy Dataset\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Generate a dummy dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=5, n_informative=3,\n",
        "    n_redundant=0, n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define K-Fold Cross-Validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "print(\"Fold Accuracies:\", scores)\n",
        "print(f\"Mean Accuracy: {scores.mean():.4f} ¬± {scores.std():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-yObFAywRAK",
        "outputId": "ecdc81ba-5bb4-4e3f-e30b-10e4e031b89b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold Accuracies: [0.925 0.975 0.875 0.875 0.925]\n",
            "Mean Accuracy: 0.9150 ¬± 0.0374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold setup\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "# Loop over folds\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_tensor)):\n",
        "    X_train, X_val = X_tensor[train_idx], X_tensor[val_idx]\n",
        "    y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]\n",
        "\n",
        "    # New model for each fold\n",
        "    model = MLP()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_model(model, optimizer, criterion, X_train, y_train)\n",
        "    acc = evaluate_model(model, X_val, y_val)\n",
        "    fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Report overall performance\n",
        "print(f\"\\nMean Accuracy: {np.mean(fold_accuracies):.4f} ¬± {np.std(fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opM7CqkiwZio",
        "outputId": "bdf857be-a172-4676-b203-4b2c44dd3c71"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Accuracy: 0.9500\n",
            "Fold 2 Accuracy: 1.0000\n",
            "Fold 3 Accuracy: 0.8750\n",
            "Fold 4 Accuracy: 0.9000\n",
            "Fold 5 Accuracy: 0.9000\n",
            "\n",
            "Mean Accuracy: 0.9250 ¬± 0.0447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Version: Manual K-Fold Cross-Validation\n",
        "\n",
        "In this section, we manually implement **K-Fold Cross-Validation** using PyTorch to understand the full training and evaluation workflow.\n",
        "\n",
        "**Goal:**  \n",
        "Perform repeated training and validation across K folds and average the results to obtain a more reliable estimate of the model‚Äôs performance.\n",
        "\n",
        "**Steps:**\n",
        "1. Create a dummy dataset using `sklearn.datasets.make_classification`.\n",
        "2. Convert it into PyTorch tensors.\n",
        "3. Split the dataset using `KFold`.\n",
        "4. Train a simple MLP model on each fold.\n",
        "5. Evaluate and report mean accuracy across folds.\n"
      ],
      "metadata": {
        "id": "0VkBwnpzwbos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dDaB-0KWwb2B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dummy dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=5, n_informative=3,\n",
        "    n_redundant=0, n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "wxpWdc9Pwd-0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=16, output_dim=2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "P4YGaYtywfZf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train_model(model, optimizer, criterion, X_train, y_train, epochs=50):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_val)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        acc = (preds == y_val).float().mean().item()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "hNJAswpuwhBc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9t8ohtxbwj06"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}