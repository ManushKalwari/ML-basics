{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy implementation"
      ],
      "metadata": {
        "id": "2KWnGZKefxPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----- utils -----\n",
        "def one_hot(y, C):\n",
        "    Y = np.zeros((y.size, C))\n",
        "    Y[np.arange(y.size), y] = 1\n",
        "    return Y\n",
        "\n",
        "def softmax(logits):\n",
        "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def relu(x): return np.maximum(0, x)\n",
        "def relu_grad(x): return (x > 0).astype(x.dtype)\n",
        "\n",
        "def ce_loss(probs, Y_true_onehot):\n",
        "    # mean NLL\n",
        "    eps = 1e-12\n",
        "    return -np.mean(np.sum(Y_true_onehot * np.log(probs + eps), axis=1))"
      ],
      "metadata": {
        "id": "ybk8LHHSfxCo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- tiny dataset (2 Gaussians) -----\n",
        "def make_blobs(n_per_class=200, dim=2, offset=2.0, seed=0):\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    c0 = rng.normal(loc=-offset, scale=1.0, size=(n_per_class, dim))\n",
        "    c1 = rng.normal(loc=+offset, scale=1.0, size=(n_per_class, dim))\n",
        "    X = np.vstack([c0, c1]).astype(np.float32)\n",
        "    y = np.array([0]*n_per_class + [1]*n_per_class, dtype=np.int64)\n",
        "\n",
        "    # shuffle\n",
        "    idx = rng.permutation(len(X))\n",
        "    return X[idx], y[idx]"
      ],
      "metadata": {
        "id": "_xeaIIG-f0Hk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- mini-batch iterator -----\n",
        "def iterate_minibatches(X, y, batch_size, shuffle=True, rng=None):\n",
        "    N = len(X)\n",
        "    idx = np.arange(N)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = np.random.default_rng() if rng is None else rng\n",
        "        rng.shuffle(idx)\n",
        "\n",
        "    for start in range(0, N, batch_size):\n",
        "        sl = idx[start:start+batch_size]\n",
        "        yield X[sl], y[sl]"
      ],
      "metadata": {
        "id": "ArvWDAXHf6Ua"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- model -----\n",
        "class TwoLayerMLP:\n",
        "    def __init__(self, D_in, H, D_out, seed=0):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.W1 = rng.normal(0, 0.02, size=(D_in, H)).astype(np.float32)\n",
        "        self.b1 = np.zeros(H, dtype=np.float32)\n",
        "        self.W2 = rng.normal(0, 0.02, size=(H, D_out)).astype(np.float32)\n",
        "        self.b2 = np.zeros(D_out, dtype=np.float32)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # cache for backprop\n",
        "        self.X = X                                  # (B, D_in)\n",
        "        self.z1 = X @ self.W1 + self.b1             # (B, H)\n",
        "        self.h  = relu(self.z1)                     # (B, H)\n",
        "        self.logits = self.h @ self.W2 + self.b2    # (B, D_out)\n",
        "        self.probs = softmax(self.logits)           # (B, D_out)\n",
        "        return self.probs\n",
        "\n",
        "    def backward(self, Y_onehot):\n",
        "        B = Y_onehot.shape[0]\n",
        "        # dL/dlogits = (p - y)/B  (softmax + CE)\n",
        "        dlogits = (self.probs - Y_onehot) / B                      # (B, C)\n",
        "        dW2 = self.h.T @ dlogits                                   # (H, C)\n",
        "        db2 = np.sum(dlogits, axis=0)                              # (C,)\n",
        "        dh  = dlogits @ self.W2.T                                  # (B, H)\n",
        "        dz1 = dh * relu_grad(self.z1)                              # (B, H)\n",
        "        dW1 = self.X.T @ dz1                                       # (D, H)\n",
        "        db1 = np.sum(dz1, axis=0)                                  # (H,)\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def step(self, grads, lr=1e-1, weight_decay=0.0):\n",
        "        dW1, db1, dW2, db2 = grads\n",
        "        # L2 weight decay (classic)\n",
        "        if weight_decay != 0.0:\n",
        "            dW1 += weight_decay * self.W1\n",
        "            dW2 += weight_decay * self.W2\n",
        "        self.W1 -= lr * dW1\n",
        "        self.b1 -= lr * db1\n",
        "        self.W2 -= lr * dW2\n",
        "        self.b2 -= lr * db2\n",
        "\n",
        "    def predict(self, X):\n",
        "        z1 = relu(X @ self.W1 + self.b1)\n",
        "        logits = z1 @ self.W2 + self.b2\n",
        "        return np.argmax(softmax(logits), axis=1)"
      ],
      "metadata": {
        "id": "qXKriU9XgAJW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- train / eval -----\n",
        "def accuracy(model, X, y):\n",
        "    return np.mean(model.predict(X) == y)\n",
        "\n",
        "def train():\n",
        "    # data\n",
        "    X, y = make_blobs(n_per_class=400, dim=2, offset=2.0, seed=42)\n",
        "    N = len(X)\n",
        "    split = int(0.8 * N)\n",
        "    Xtr, ytr = X[:split], y[:split]\n",
        "    Xva, yva = X[split:], y[split:]\n",
        "    Ytr_oh, Yva_oh = one_hot(ytr, 2), one_hot(yva, 2)\n",
        "\n",
        "    # model + hyperparams\n",
        "    model = TwoLayerMLP(D_in=2, H=32, D_out=2, seed=1)\n",
        "    lr = 0.1\n",
        "    wd = 1e-4\n",
        "    batch_size = 64\n",
        "    epochs = 50\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "\n",
        "        # training\n",
        "        for Xb, yb in iterate_minibatches(Xtr, ytr, batch_size, shuffle=True):\n",
        "            Yb = one_hot(yb, 2)\n",
        "            probs = model.forward(Xb)\n",
        "            grads = model.backward(Yb)\n",
        "            model.step(grads, lr=lr, weight_decay=wd)\n",
        "\n",
        "        # eval (no batches needed here)\n",
        "        probs_tr = model.forward(Xtr)\n",
        "        loss_tr = ce_loss(probs_tr, Ytr_oh)\n",
        "        acc_tr = accuracy(model, Xtr, ytr)\n",
        "\n",
        "        probs_va = model.forward(Xva)\n",
        "        loss_va = ce_loss(probs_va, Yva_oh)\n",
        "        acc_va = accuracy(model, Xva, yva)\n",
        "\n",
        "        if ep % 5 == 0 or ep == 1:\n",
        "            print(f\"epoch {ep:02d} | \"\n",
        "                  f\"train loss {loss_tr:.4f} acc {acc_tr:.3f} | \"\n",
        "                  f\"val loss {loss_va:.4f} acc {acc_va:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NpvuX4Z5godr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FMqZMZigotF",
        "outputId": "4b5ed6f6-9396-4ec9-b56b-2c119a90c2bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train loss 0.6631 acc 0.995 | val loss 0.6643 acc 0.994\n",
            "epoch 05 | train loss 0.0736 acc 0.991 | val loss 0.0807 acc 0.988\n",
            "epoch 10 | train loss 0.0292 acc 0.991 | val loss 0.0343 acc 0.988\n",
            "epoch 15 | train loss 0.0218 acc 0.991 | val loss 0.0263 acc 0.988\n",
            "epoch 20 | train loss 0.0189 acc 0.994 | val loss 0.0232 acc 0.988\n",
            "epoch 25 | train loss 0.0175 acc 0.994 | val loss 0.0216 acc 0.988\n",
            "epoch 30 | train loss 0.0167 acc 0.994 | val loss 0.0206 acc 0.988\n",
            "epoch 35 | train loss 0.0161 acc 0.994 | val loss 0.0200 acc 0.988\n",
            "epoch 40 | train loss 0.0158 acc 0.994 | val loss 0.0196 acc 0.988\n",
            "epoch 45 | train loss 0.0155 acc 0.994 | val loss 0.0193 acc 0.988\n",
            "epoch 50 | train loss 0.0153 acc 0.994 | val loss 0.0191 acc 0.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch implementation"
      ],
      "metadata": {
        "id": "OBupGU1rg-8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 2-layer MLP (ReLU -> Dropout -> Linear), CE loss, mini-batches"
      ],
      "metadata": {
        "id": "gBcTNR-4g-Md"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "gKvqowIihF15"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- tiny synthetic data (binary classification) ----\n",
        "N, Din, Dout = 800, 2, 2\n",
        "\n",
        "X0 = torch.randn(N//2, Din) - 2.0\n",
        "X1 = torch.randn(N//2, Din) + 2.0\n",
        "X = torch.cat([X0, X1], dim=0)\n",
        "\n",
        "y = torch.cat([torch.zeros(N//2, dtype=torch.long),\n",
        "               torch.ones(N//2,  dtype=torch.long)], dim=0)\n",
        "\n",
        "perm = torch.randperm(N);\n",
        "X, y = X[perm], y[perm]\n",
        "\n",
        "Xtr, ytr = X[:int(0.8*N)], y[:int(0.8*N)]\n",
        "Xva, yva = X[int(0.8*N):], y[int(0.8*N):]"
      ],
      "metadata": {
        "id": "wLeQr26_hJ5D"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, Din, H, Dout, p=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # weights as Parameters (so autograd tracks them)\n",
        "        self.W1 = nn.Parameter(torch.randn(Din, H) * 0.02)\n",
        "        self.b1 = nn.Parameter(torch.zeros(H))\n",
        "        self.W2 = nn.Parameter(torch.randn(H, Dout) * 0.02)\n",
        "        self.b2 = nn.Parameter(torch.zeros(Dout))\n",
        "        self.p = p\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x @ self.W1 + self.b1\n",
        "        h = F.relu(h)\n",
        "        h = F.dropout(h, p=self.p, training=self.training)  # Dropout only in train mode\n",
        "        logits = h @ self.W2 + self.b2\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CuXVX9gehKW8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoLayerMLP(Din=Din, H=32, Dout=Dout, p=0.5).to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)  # L2 via weight_decay\n",
        "batch_size, epochs = 64, 30"
      ],
      "metadata": {
        "id": "9NhQW0DBhRLe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_minibatches(X, y, bs):\n",
        "    for i in range(0, len(X), bs):\n",
        "        yield X[i:i+bs], y[i:i+bs]"
      ],
      "metadata": {
        "id": "QZBdN46dhU4P"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- training loop ----\n",
        "for ep in range(1, epochs+1):\n",
        "    model.train()\n",
        "\n",
        "    for xb, yb in iterate_minibatches(Xtr.to(device), ytr.to(device), batch_size):\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)  # softmax+NLL under the hood\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        def eval_split(Xs, ys):\n",
        "            logits = model(Xs.to(device))\n",
        "            loss = F.cross_entropy(logits, ys.to(device)).item()\n",
        "            acc = (logits.argmax(1).cpu() == ys).float().mean().item()\n",
        "            return loss, acc\n",
        "\n",
        "        tr_loss, tr_acc = eval_split(Xtr, ytr)\n",
        "        va_loss, va_acc = eval_split(Xva, yva)\n",
        "\n",
        "    if ep in {1, 5, 10, 20, 30}:\n",
        "        print(f\"epoch {ep:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
        "              f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0SnabOhbzY",
        "outputId": "eac4f027-658a-421a-8582-851748b5df1b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train loss 0.6560 acc 0.983 | val loss 0.6565 acc 1.000\n",
            "epoch 05 | train loss 0.0813 acc 0.994 | val loss 0.0681 acc 1.000\n",
            "epoch 10 | train loss 0.0305 acc 0.994 | val loss 0.0162 acc 1.000\n",
            "epoch 20 | train loss 0.0209 acc 0.994 | val loss 0.0055 acc 1.000\n",
            "epoch 30 | train loss 0.0188 acc 0.994 | val loss 0.0033 acc 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYpI8C7Hhcbw"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}